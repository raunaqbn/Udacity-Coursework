{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "The aim of linear regression is to draw a line of best fit. Typically gradient descent is used to find this line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original error is 5565.107834483211\n",
      "After running 2000 iterations we get m = 0.1483551275485007 , c = 1.4765762305610064 and the error = 112.57949073360243 \n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "#Error for the line is defined as 1/(len(points))sigma(y(i)-(mx(i)+b))^2\n",
    "def calc_error_from_points(c, m, points):\n",
    "    totalError = 0\n",
    "    #loop over all the points to get the error\n",
    "    for i in range(0,len(points)):\n",
    "        y = points[i,1]\n",
    "        x = points[i,0]\n",
    "        totalError += (y - (m*x + c))**2\n",
    "        \n",
    "    return totalError/ float(len(points))\n",
    "\n",
    "#This function calculates the partial derivative of the error function and uses gradient descent\n",
    "# gradient descent runs over a bunch of iterations to slowly move towards a local minima by taking one \n",
    "#step per iteration in the direction where the error reduces.\n",
    "def step_gradient_descent(c, m, points, learning_rate):\n",
    "    N = float(len(points))\n",
    "    m_gradient = 0\n",
    "    c_gradient = 0\n",
    "    #loop over the points to calculate the c and m gradient values\n",
    "    for i in range(0,len(points)):\n",
    "        y = points[i,1]\n",
    "        x = points[i,0]\n",
    "        \n",
    "        #calculate the partial derivative for the error func for the parameters\n",
    "        m_gradient += (-2/N)*x*(y - (m*x + c))\n",
    "        c_gradient += (-2/N)*(y - (m*x + c))\n",
    "        \n",
    "    new_m = m - learning_rate * m_gradient\n",
    "    new_c = c - learning_rate * c_gradient\n",
    "    \n",
    "    return [new_m, new_c]\n",
    "\n",
    "# run the gradient descent algorithm for 1000 iterations to get the final value of c and m\n",
    "def gradient_descend_runner(learning_rate, num_iterations, initial_m, initial_c, points):\n",
    "    c = initial_c\n",
    "    m = initial_m\n",
    "    for i in range(num_iterations):\n",
    "        [m,c] = step_gradient_descent(c,m,array(points),learning_rate)\n",
    "    return [c,m]\n",
    "\n",
    "def run():\n",
    "    # get the points for the numver of hours studied vs test scores\n",
    "    points = genfromtxt('data.csv', delimiter = ',')\n",
    "    \n",
    "    #Hyperparameters:\n",
    "    #A machine learning model is the definition of a mathematical formula with a number of parameters \n",
    "    #that need to be learned from the data. That is the crux of machine learning: fitting a model to the data. \n",
    "    #This is done through a process known as model training. In other words, by training a model with existing data,\n",
    "    #we are able to fit the model parameters.However, there is another kind of parameters that cannot be\n",
    "    #directly learned from the regular training process. These parameters express “higher-level” properties \n",
    "    #of the model such as its complexity or how fast it should learn. They are called hyperparameters. \n",
    "    #Hyperparameters are usually fixed before the actual training process begins\n",
    "    learning_rate = 0.0001\n",
    "    \n",
    "    # define the parameters which the model is based upon\n",
    "    #y = mx + c. Here we need to fit the data with a linear line and need to learn the slope m and the y-intercept c\n",
    "    initial_m = 0\n",
    "    initial_c = 0\n",
    "    \n",
    "    #number of iterations for training the model. Since we have very few data points we will need only 1000 iterations\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    #Train the model to learn the parameters\n",
    "    [c,m] = gradient_descend_runner(learning_rate, num_iterations, initial_m, initial_c, points)\n",
    "    original_error = calc_error_from_points(initial_c, initial_m, points)\n",
    "    print (\"The original error is {}\".format(original_error))\n",
    "    error = calc_error_from_points(c,m,points)\n",
    "    print(\"After running {} iterations we get m = {} , c = {} and the error = {} \".format(num_iterations,c,m,error))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
